{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from huggingface_hub import login\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import get_json_schema\n",
    "\n",
    "import tools\n",
    "from checker import check_success_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"google/functiongemma-270m-it\"\n",
    "learning_rate = 5e-5\n",
    "\n",
    "TOOLS = [get_json_schema(tool) for name, tool in tools.__dict__.items() if callable(tool) and getattr(tool, \"__module__\", \"\") == tools.__name__]\n",
    "DEFAULT_SYSTEM_MSG = (\n",
    "    \"Du bist ein hilfreicher Assistent mit Zugriff auf spezifische Werkzeuge.\\n\",\n",
    "    \"1. Prüfe zuerst, ob eine spezifische Funktion (wie Wetter, Musik, Alarm) die Anfrage lösen kann.\\n\",\n",
    "    \"2. Wenn keine spezifische Funktion passt, nutze die Websuche ('search_web'), um Informationen zu finden.\\n\",\n",
    "    \"3. Nutze nur dann reinen Text, wenn gar keine Funktion passt (z.B. bei Begrüßungen). Antworte immer im validen Funktionsaufruf-Format oder auf Deutsch.\\n\")\n",
    "\n",
    "def create_conversation(sample, tool_names=None):\n",
    "    tool_name = sample.get(\"tool_name\")\n",
    "    \n",
    "    if tool_names and isinstance(tool_name, int):\n",
    "        tool_name = tool_names[tool_name]\n",
    "\n",
    "    if tool_name and tool_name != \"null\":\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"tool_calls\": [{\n",
    "                \"type\": \"function\", \n",
    "                \"function\": {\n",
    "                    \"name\": tool_name, \n",
    "                    \"arguments\": sample[\"tool_arguments\"]\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        response_text = sample.get(\"response\", \"Ich kann dir dabei leider nicht helfen.\")\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response_text\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"developer\", \"content\": DEFAULT_SYSTEM_MSG},\n",
    "            {\"role\": \"user\", \"content\": sample[\"user_content\"]},\n",
    "            assistant_message,\n",
    "        ],\n",
    "        \"tools\": TOOLS\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "loaded_json = []\n",
    "for file_path in glob.glob(os.path.abspath(\"data/*.json\")):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        loaded_json.extend(json.load(f))\n",
    "\n",
    "dataset = Dataset.from_list(loaded_json)\n",
    "\n",
    "# Cast tool_name to ClassLabel for stratified splitting\n",
    "tool_names = sorted(list(set(item[\"tool_name\"] for item in loaded_json)))\n",
    "dataset = dataset.cast_column(\"tool_name\", ClassLabel(names=tool_names))\n",
    "\n",
    "# Map without removing columns to allow stratification on tool_name\n",
    "original_columns = dataset.column_names\n",
    "dataset = dataset.map(create_conversation, fn_kwargs={\"tool_names\": tool_names}, batched=False)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, stratify_by_column=\"tool_name\")\n",
    "\n",
    "# Remove original columns after splitting\n",
    "dataset[\"train\"] = dataset[\"train\"].remove_columns(original_columns)\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns(original_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer \n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "print(f\"Device: {model.device}\")\n",
    "print(f\"DType: {model.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Initial check before training ---\")\n",
    "check_success_rate(dataset[\"test\"], model, tokenizer, TOOLS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "torch_dtype = model.dtype\n",
    "args = SFTConfig(\n",
    "    output_dir=\"./functiongemma-tool-calling-sft\",              # directory to save and repository id\n",
    "    max_length=1024,                         # max sequence length for model and packing of the dataset\n",
    "    packing=False,                          # Groups multiple samples in the dataset into a single sequence\n",
    "    num_train_epochs=64,                     # number of training epochs\n",
    "    per_device_train_batch_size=4,          # batch size per device during training\n",
    "    gradient_checkpointing=False,           # Caching is incompatible with gradient checkpointing\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=1,                        # log every step\n",
    "    #save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    eval_strategy=\"epoch\",                  # evaluate checkpoint every epoch\n",
    "    learning_rate=learning_rate,            # learning rate\n",
    "    fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "    bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "    lr_scheduler_type=\"constant\",            # use constant learning rate scheduler\n",
    "    push_to_hub=False,                        # push model to hub\n",
    "    report_to=\"tensorboard\",                 # report metrics to tensorboard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer object\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training, the model will be automatically saved to the Hub and the output directory\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Check after training ---\")\n",
    "check_success_rate(dataset[\"test\"], model, tokenizer, TOOLS)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
